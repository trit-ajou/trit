import math
import torch
import os
import numpy as np
import gc
import torch.nn.functional as F
from torch.amp.grad_scaler import GradScaler
from torch import autocast
from ..datas.Dataset import MangaDataset3
from ..datas.TextedImage import TextedImage
from torch import FloatTensor, nn
from torchvision import transforms # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€ ì„í¬íŠ¸
from transformers.optimization import Adafactor
from tqdm import tqdm

from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.models.transformers.transformer_sd3 import SD3Transformer2DModel
from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3 import StableDiffusion3Pipeline
from peft import get_peft_model, LoraConfig, PeftModel
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
def identity_collate(batch_images):
    return batch_images

# ë¶€ë™ì†Œìˆ˜ì  í–‰ë ¬ ê³±ì…ˆ ì •ë°€ë„ ì„¤ì •
torch.set_float32_matmul_precision('high')
print("ë¶€ë™ì†Œìˆ˜ì  í–‰ë ¬ ê³±ì…ˆ ì •ë°€ë„ë¥¼ 'high'ë¡œ ì„¤ì •")

class Model3(nn.Module):
    def __init__(self, model_config: dict, device: str = "cuda"):
        super().__init__()
        self.model_config = model_config
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
    
    def lora_train(self, texted_images_for_model3: list[TextedImage]):
        print("Loading SD3 pipeline for LoRA training...")

        # í•™ìŠµì€ fp16ìœ¼ë¡œ ì„¤ì •
        weight_dtype = torch.float16
        model_id = self.model_config["model_id"]
        lora_weights_path = self.model_config["lora_path"]
        num_epochs = self.model_config.get("epochs", 10)
        batch_size = self.model_config.get("batch_size", 4)
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        prompt = self.model_config.get("prompts", "pure black and white manga style image with no color tint, absolute grayscale, contextual manga style")
        negative_prompt = self.model_config.get("negative_prompt", "photo, realistic, color, colorful, purple, violet, sepia, any color tint, blurry")
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë”©
        try:
            pipe = StableDiffusion3Pipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
            )
            pipe.to(self.device)
            print(f"SD3 pipeline loaded successfully to {self.device}.")
            
            # í•„ìš”í•œ ë¶€ë¶„ ë¡œë“œ
            vae = pipe.vae
            text_encoder = pipe.text_encoder
            text_encoder_2 = pipe.text_encoder_2
            text_encoder_3 = pipe.text_encoder_3
            transformer = pipe.transformer
            tokenizer = pipe.tokenizer
            tokenizer_2 = pipe.tokenizer_2
            tokenizer_3 = pipe.tokenizer_3
            print("SD3 components loaded successfully.")
            
        except Exception as e:
            print(f"Error loading SD3 pipeline: {e}")
            return
        
        # pipe ê°ì²´ ì§€ìš°ê¸°
        del pipe
        gc.collect(); torch.cuda.empty_cache()
        
        try:
            #ê¸°ì¡´ lora ê°€ì¤‘ì¹˜ ë¡œë“œ
            if os.path.exists(lora_weights_path) and os.path.exists(os.path.join(lora_weights_path, "best_model.safetensors")):
                transformer_lora = PeftModel.from_pretrained(transformer, lora_weights_path)
                print(f"LoRA weights loaded from {lora_weights_path}")
            else:

                lora_config = LoraConfig(
                    r = self.model_config.get("lora_rank", 8),
                    lora_alpha = self.model_config.get("lora_alpha", 16),
                    target_modules = ["to_k", "to_q", "to_v", "to_out.0"],
                    lora_dropout = 0.05,
                    bias = "none",
                    init_lora_weights = "gaussian",
                )
                transformer_lora = get_peft_model(transformer, lora_config)
            # vae ë° í…ìŠ¤íŠ¸ ì¸ì½”ë” ê°€ì¤‘ì¹˜ ë™ê²°
            vae.requires_grad_(False)
            text_encoder.requires_grad_(False)
            if text_encoder_2: text_encoder_2.requires_grad_(False)
            if text_encoder_3: text_encoder_3.requires_grad_(False)
        except Exception as e:
            print(f"Error setting up LoRA: {e}")
            return
        
        # ë°ì´í„°ì…‹ì„ í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• 
        train_size = int(0.8 * len(texted_images_for_model3))
        val_size = len(texted_images_for_model3) - train_size
        
        full_train_set = MangaDataset3(texted_images_for_model3)
        
        train_sets, valid_sets = torch.utils.data.random_split(full_train_set, [train_size, val_size])
        
        train_loader = torch.utils.data.DataLoader(
            train_sets,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=identity_collate,
        )
        val_loader = torch.utils.data.DataLoader(
            valid_sets,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=identity_collate,
        )
        print(f"ë°ì´í„°ì…‹ ë¶„í• : í›ˆë ¨ {len(train_loader)}ê°œ, ê²€ì¦ {len(val_loader)}ê°œ")
        
        # optimizer ì„¤ì • - Adafactor ìì²´ ìŠ¤ì¼€ì¼ë§ ì‚¬ìš©
        optimizer = Adafactor(
            transformer_lora.parameters(),
            lr= None, 
            scale_parameter=True,
            relative_step=True,
            warmup_init=True,  
        )
        
        print("[Model3] Optimizer set up.")
        
        
        # LoRA í•™ìŠµ ë£¨í”„
        output_dir = self.model_config.get("output_dir", "datas/images/output/model3_sd3_lora")
        os.makedirs(output_dir, exist_ok=True)
        
        print("[Model3] Lora í•™ìŠµ ì‹œì‘")
        # ìµœê³  ê²€ì¦ ì†ì‹¤ ì¶”ì 
        best_val_loss = float('inf')
        # CUDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™” (ë°˜ë³µì ì¸ í¬ê¸°ì˜ ì…ë ¥ì— ëŒ€í•´ ìµœì í™”)
        torch.backends.cudnn.benchmark = True
        scaler = GradScaler(enabled=(weight_dtype == torch.float16))
        
        # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        noise_scheduler = DDIMScheduler(
            num_train_timesteps=1000,
            beta_start=0.00085,
            beta_end=0.012,
            beta_schedule="scaled_linear",
            clip_sample=False,
            set_alpha_to_one=False
            )
        
        for epoch in tqdm(range(num_epochs), desc="Epochs"):
            transformer_lora.train()
            epoch_loss = 0.0
            
            for batch_images in tqdm(train_loader, desc="Training batches"):
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
                
                # VAE ì…ë ¥ì€ float32ë¥¼ ì„ í˜¸. img.origëŠ” [0,1] ì´ì–´ì•¼í•¨í•¨
                original_pixel_values_batch = torch.stack(
                    [img.orig for img in batch_images]
                ).to(self.device, dtype=torch.float32) 

                mask_pixel_values_batch = torch.stack(
                    [img.mask for img in batch_images]
                ).to(self.device, dtype=weight_dtype)   
                
                with torch.no_grad():
                    # VAE ì¸ì½”ë”©
                    vae.to(self.device)
                    
                    
                    target_latents_batch = vae.encode(original_pixel_values_batch).latent_dist.sample() * vae.config.scaling_factor
                    target_latents_batch = target_latents_batch.to(dtype=weight_dtype)
                    
                    latent_mask_batch = F.interpolate(
                        mask_pixel_values_batch,
                        size=target_latents_batch.shape[-2:],
                        mode="nearest"
                    ) # [B, 1, H_lat, W_lat]
                    
                      
                    # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± - ë°°ì¹˜ í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ ìƒì„±
                    print("[Model3 TRAIN] í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ ...")
                    text_encoder.to(self.device)
                    if text_encoder_2: text_encoder_2.to(self.device)
                    if text_encoder_3: text_encoder_3.to(self.device)

                    prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds = self._encode_prompt(
                        prompt, negative_prompt, tokenizer, tokenizer_2, tokenizer_3,
                        text_encoder, text_encoder_2, text_encoder_3, self.device, len(batch_images)
                    )

                    # ì„ë² ë”© ì¤€ë¹„ - CFGë¥¼ ìœ„í•´ negativeì™€ positive ê²°í•©
                    prompt_embeds_full = torch.cat([negative_prompt_embeds.to(weight_dtype), prompt_embeds.to(weight_dtype)], dim=0)
                    pooled_embeddings_full = torch.cat([negative_pooled_prompt_embeds.to(weight_dtype), pooled_prompt_embeds.to(weight_dtype)], dim=0)


                noise_batch = torch.randn_like(target_latents_batch)
                timesteps_batch = torch.randint(0, noise_scheduler.num_train_timesteps, (len(batch_images), ), device=self.device).long()
                
                # íƒ€ê²Ÿ ì ì¬ ë²¡í„°ì— ë…¸ì´ì¦ˆ ì¶”ê°€ê°€
                noisy_target_latents = noise_scheduler.add_noise(target_latents_batch, noise_batch, timesteps_batch) # type: ignore
                # [B, C_lat, H_lat, W_lat] 
                #ì…ë ¥ ëª¨ë¸ êµ¬ì„±
                initial_lantents = target_latents_batch * (1 - latent_mask_batch) + noisy_target_latents * latent_mask_batch
                    
                    
                with autocast("cuda",dtype=weight_dtype):
                    print("[Model3 TRAIN] ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ì¤‘ ...")
                    # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡
                    latent_model_input = torch.cat([initial_lantents]*2, dim=0)
                    timesteps_input = torch.cat([timesteps_batch] * 2, dim = 0)
                    
                    noise_pred = transformer_lora(
                        hidden_states=latent_model_input,
                        timestep=timesteps_input,
                        encoder_hidden_states=prompt_embeds_full,
                        pooled_projections=pooled_embeddings_full,
                        return_dict=False
                    )[0]
                    
                    #noise_pred ë¶„ë¦¬ë¦¬
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    # ì†ì‹¤ ê³„ì‚°
                    loss = F.mse_loss(noise_pred_text.to(torch.float32), noise_batch.to(torch.float32), reduction="none")
                    # ì†ì‹¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
                    mask_weight = self.model_config.get("mask_weight", 2.0)
                    unmask_weight = 1.0
                    weight_map_per_element = (
                        latent_mask_batch.to(loss.device, dtype=torch.float32) * (mask_weight - unmask_weight) 
                        + unmask_weight
                    )
                    weighted_loss_map = loss * weight_map_per_element
                    loss = weighted_loss_map.mean()
                
                
                
                optimizer.zero_grad(set_to_none=True)
            
                scaler.scale(loss).backward() # ğŸš€ ìŠ¤ì¼€ì¼ëœ ì†ì‹¤ë¡œ ì—­ì „íŒŒ

                # ğŸš€ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ì „, unscale í›„)
                scaler.unscale_(optimizer) # ì˜µí‹°ë§ˆì´ì €ì— ì—°ê²°ëœ íŒŒë¼ë¯¸í„°ë“¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì›ë˜ ê°’ìœ¼ë¡œ ë˜ëŒë¦¼
                torch.nn.utils.clip_grad_norm_(
                    [p for p in transformer_lora.parameters() if p.requires_grad],
                    max_norm=self.model_config.get("max_grad_norm", 1.0) 
                )
                
                scaler.step(optimizer) # ğŸš€ ì˜µí‹°ë§ˆì´ì € ìŠ¤í… (ìŠ¤ì¼€ì¼ëœ ê·¸ë˜ë””ì–¸íŠ¸ ìë™ ì²˜ë¦¬)
                scaler.update()        # ğŸš€ ìŠ¤ì¼€ì¼ëŸ¬ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ìŠ¤ì¼€ì¼ ì¡°ì •)
            
                
                # ì†ì‹¤ ì¶”ì 
                epoch_loss += loss.detach().item()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del loss, noise_pred, latent_model_input, timesteps_input
                torch.cuda.empty_cache()
                
            # ì—í­ ì¢…ë£Œ í›„ ê²€ì¦ ì†ì‹¤ ê³„ì‚°
            if epoch > 0:
                avg_train_loss = epoch_loss / epoch
                print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}")
                
                # ê²€ì¦ ì†ì‹¤ ê³„ì‚°
                val_loss = self._calculate_validation_loss(
                    transformer_lora, vae, text_encoder, text_encoder_2, text_encoder_3,
                    tokenizer, tokenizer_2, tokenizer_3, noise_scheduler,
                    val_loader, weight_dtype
                )
                print(f"Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss:.4f}")
                
                # ì—í­ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
                # ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ë©´ best_model ì €ì¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    print(f"ìƒˆë¡œìš´ ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}, ëª¨ë¸ ì €ì¥ ì¤‘...")

                    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
                    os.makedirs(lora_weights_path, exist_ok=True)
                    transformer_lora.save_pretrained(lora_weights_path)
                    print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ.")

    def _calculate_validation_loss(self, transformer_lora, vae, text_encoder, text_encoder_2, text_encoder_3,
                             tokenizer, tokenizer_2, tokenizer_3, noise_scheduler,
                             val_loader, weight_dtype):
        """
        ì¸í˜ì¸íŒ… í•™ìŠµ ë°©ì‹ì— ë§ê²Œ ìˆ˜ì •ëœ ê²€ì¦ ì†ì‹¤ ê³„ì‚° í•¨ìˆ˜
        """
        transformer_lora.eval()
        device = self.device
        total_val_loss = 0.0
        num_val_batches = 0
        
        prompt = self.model_config.get("prompts", "pure black and white manga style image with no color tint, absolute grayscale, contextual manga style")
        negative_prompt = self.model_config.get("negative_prompt", "photo, realistic, color, colorful, purple, violet, sepia, any color tint, blurry")
        
        with torch.no_grad():
            for batch_images in val_loader:
                torch.cuda.empty_cache()
                
                original_pixel_values = torch.stack([img.orig for img in batch_images]).to(device, dtype=torch.float32)
                mask_pixel_values = torch.stack([img.mask for img in batch_images]).to(device, dtype=weight_dtype)
                
                vae.to(device)
                target_latents = vae.encode(original_pixel_values).latent_dist.sample() * vae.config.scaling_factor
                target_latents = target_latents.to(dtype=weight_dtype)
                
                latent_mask = F.interpolate(
                    mask_pixel_values, size=target_latents.shape[-2:], mode="nearest"
                )
                
                # KEY CHANGE: ê²€ì¦ì—ì„œë„ í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
                # 1. íƒ€ê²Ÿ ë…¸ì´ì¦ˆì™€ íƒ€ì„ìŠ¤í… ìƒì„±
                noise = torch.randn_like(target_latents)
                timesteps = torch.randint(
                    0, noise_scheduler.num_train_timesteps, (len(batch_images),), device=device
                ).long()
                
                # 2. ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ íƒ€ê²Ÿ ì ì¬ ë²¡í„° ìƒì„±
                noisy_target_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)
                
                # 3. ëª¨ë¸ì˜ ì‹¤ì œ ì…ë ¥(initial_latents) êµ¬ì„±
                initial_latents = target_latents * (1 - latent_mask) + noisy_target_latents * latent_mask
                
                # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
                text_encoder.to(device)
                if text_encoder_2: text_encoder_2.to(device)
                if text_encoder_3: text_encoder_3.to(device)
                
                prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds = self._encode_prompt(
                    prompt, negative_prompt, tokenizer, tokenizer_2, tokenizer_3,
                    text_encoder, text_encoder_2, text_encoder_3, device, len(batch_images)
                )
                
                prompt_embeds_full = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0).to(dtype=weight_dtype)
                pooled_embeddings_full = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0).to(dtype=weight_dtype)
                
                # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ë° ë…¸ì´ì¦ˆ ì˜ˆì¸¡
                latent_model_input = torch.cat([initial_latents] * 2, dim=0)
                timesteps_input = torch.cat([timesteps] * 2, dim=0)
                
                noise_pred = transformer_lora(
                    hidden_states=latent_model_input,
                    timestep=timesteps_input,
                    encoder_hidden_states=prompt_embeds_full,
                    pooled_projections=pooled_embeddings_full,
                    return_dict=False
                )[0]
                
                _, noise_pred_text = noise_pred.chunk(2)
                
                # ì†ì‹¤ ê³„ì‚° (íƒ€ê²Ÿì€ ì‹¤ì œ ì¶”ê°€ëœ ë…¸ì´ì¦ˆ `noise`)
                loss = F.mse_loss(noise_pred_text.to(torch.float32), noise.to(torch.float32), reduction="none")
                
                mask_weight = self.model_config.get("mask_weight", 2.0)
                unmask_weight = 1.0
                weight_map_per_element = (
                    latent_mask.to(loss.device, dtype=torch.float32) * (mask_weight - unmask_weight) 
                    + unmask_weight
                )
                weighted_loss_map = loss * weight_map_per_element
                val_loss = weighted_loss_map.mean()
                
                total_val_loss += val_loss.item()
                num_val_batches += 1
                
                del val_loss, noise_pred, latent_model_input, timesteps_input
                torch.cuda.empty_cache()
        
        avg_val_loss = total_val_loss / num_val_batches if num_val_batches > 0 else float('inf')
        return avg_val_loss

    def _encode_prompt(self, prompt, negative_prompt, tokenizer, tokenizer_2, tokenizer_3,
                     text_encoder, text_encoder_2, text_encoder_3, device, batch_size=1):
        """
        í…ìŠ¤íŠ¸ ì¸ì½”ë”© í—¬í¼ ë©”ì„œë“œ - SD3 íŒŒì´í”„ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
        """
        # ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ ë³µì œ
        prompts = [prompt] * batch_size
        negative_prompts = [negative_prompt] * batch_size
        
        # ê³µí†µ max_length ì„¤ì •
        max_length = 77  # SD3ì˜ í‘œì¤€ ì‹œí€€ìŠ¤ ê¸¸ì´
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” 1 ì²˜ë¦¬ (CLIP ViT-L/14)
        text_inputs = tokenizer(
            prompts,  # ë°°ì¹˜ í¬ê¸° ì ìš©
            padding="max_length",
            max_length=max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids.to(device)
        
        uncond_input = tokenizer(
            negative_prompts,  # ë°°ì¹˜ í¬ê¸° ì ìš©
            padding="max_length",
            max_length=max_length,
            truncation=True,
            return_tensors="pt",
        )
        uncond_input_ids = uncond_input.input_ids.to(device)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” 1 ì¶œë ¥ (CLIP: 768 ì°¨ì›)
        encoder_output_1 = text_encoder(text_input_ids)
        
        # CLIPTextModelOutput ê°ì²´ ì²˜ë¦¬
        if hasattr(encoder_output_1, 'last_hidden_state'):
            prompt_embeds = encoder_output_1.last_hidden_state  # (B, S, 768)
        elif isinstance(encoder_output_1, tuple) and len(encoder_output_1) > 0:
            prompt_embeds = encoder_output_1[0]
        else:
            prompt_embeds = encoder_output_1  # ì§ì ‘ í…ì„œì¸ ê²½ìš°
        
        # ì°¨ì› í™•ì¸ ë° ìˆ˜ì •
        if isinstance(prompt_embeds, torch.Tensor) and len(prompt_embeds.shape) == 2:  # (B, D) í˜•íƒœì¸ ê²½ìš°
            # ì‹œí€€ìŠ¤ ì°¨ì› ì¶”ê°€ (B, D) -> (B, 1, D)
            prompt_embeds = prompt_embeds.unsqueeze(1)
            # ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ max_lengthë¡œ í™•ì¥
            prompt_embeds = prompt_embeds.repeat(1, max_length, 1)
        
        # ë™ì¼í•œ ì²˜ë¦¬ë¥¼ negative_promptì—ë„ ì ìš©
        uncond_output_1 = text_encoder(uncond_input_ids)
        
        # CLIPTextModelOutput ê°ì²´ ì²˜ë¦¬
        if hasattr(uncond_output_1, 'last_hidden_state'):
            negative_prompt_embeds = uncond_output_1.last_hidden_state
        elif isinstance(uncond_output_1, tuple) and len(uncond_output_1) > 0:
            negative_prompt_embeds = uncond_output_1[0]
        else:
            negative_prompt_embeds = uncond_output_1
        
        if isinstance(negative_prompt_embeds, torch.Tensor) and len(negative_prompt_embeds.shape) == 2:
            negative_prompt_embeds = negative_prompt_embeds.unsqueeze(1)
            negative_prompt_embeds = negative_prompt_embeds.repeat(1, max_length, 1)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” 2 ì²˜ë¦¬ (OpenCLIP ViT-bigG/14)
        pooled_prompt_embeds = None
        negative_pooled_prompt_embeds = None
        
        if text_encoder_2 is not None:
            try:
                text_inputs_2 = tokenizer_2(
                    prompts,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt",
                )
                text_input_ids_2 = text_inputs_2.input_ids.to(device)
                
                uncond_input_2 = tokenizer_2(
                    negative_prompts,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt",
                )
                uncond_input_ids_2 = uncond_input_2.input_ids.to(device)
                
                # í…ìŠ¤íŠ¸ ì¸ì½”ë” 2 ì¶œë ¥ (OpenCLIP)
                encoder_output_2 = text_encoder_2(text_input_ids_2)
                uncond_output_2 = text_encoder_2(uncond_input_ids_2)
                
                # ì¶œë ¥ í˜•íƒœ í™•ì¸ ë° ì²˜ë¦¬ - CLIPTextModelOutput ê°ì²´ ì²˜ë¦¬
                if hasattr(encoder_output_2, 'last_hidden_state') and hasattr(encoder_output_2, 'pooler_output'):
                    text_embeds = encoder_output_2.last_hidden_state
                    pooled_prompt_embeds = encoder_output_2.pooler_output
                    
                    negative_text_embeds = uncond_output_2.last_hidden_state
                    negative_pooled_prompt_embeds = uncond_output_2.pooler_output
                elif isinstance(encoder_output_2, tuple) and len(encoder_output_2) > 1:
                    text_embeds = encoder_output_2[0]
                    pooled_prompt_embeds = encoder_output_2[1]
                    
                    negative_text_embeds = uncond_output_2[0]
                    negative_pooled_prompt_embeds = uncond_output_2[1]
                else:
                    text_embeds = encoder_output_2 if not hasattr(encoder_output_2, 'last_hidden_state') else encoder_output_2.last_hidden_state
                    negative_text_embeds = uncond_output_2 if not hasattr(uncond_output_2, 'last_hidden_state') else uncond_output_2.last_hidden_state
                    
                    # pooled ì¶œë ¥ì´ ì—†ëŠ” ê²½ìš° ì‹œí€€ìŠ¤ ì¶œë ¥ì˜ ì²« ë²ˆì§¸ í† í° ì‚¬ìš©
                    if isinstance(text_embeds, torch.Tensor):
                        pooled_prompt_embeds = text_embeds[:, 0]
                        negative_pooled_prompt_embeds = negative_text_embeds[:, 0]
                
                # ì°¨ì› í™•ì¸ - pooled ì¶œë ¥ì€ (B, D) í˜•íƒœì—¬ì•¼ í•¨
                if isinstance(pooled_prompt_embeds, torch.Tensor) and len(pooled_prompt_embeds.shape) == 3:
                    # ì²« ë²ˆì§¸ í† í°ë§Œ ì‚¬ìš© (B, S, D) -> (B, D)
                    pooled_prompt_embeds = pooled_prompt_embeds[:, 0]
                    negative_pooled_prompt_embeds = negative_pooled_prompt_embeds[:, 0]
                    
            except Exception as e:
                print(f"í…ìŠ¤íŠ¸ ì¸ì½”ë” 2 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” 3 ì²˜ë¦¬ (T5-XL)
        if text_encoder_3 is not None:
            try:
                text_inputs_3 = tokenizer_3(
                    prompts,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt",
                )
                text_input_ids_3 = text_inputs_3.input_ids.to(device)
                
                uncond_input_3 = tokenizer_3(
                    negative_prompts,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt",
                )
                uncond_input_ids_3 = uncond_input_3.input_ids.to(device)
                
                # í…ìŠ¤íŠ¸ ì¸ì½”ë” 3 ì¶œë ¥ (T5)
                encoder_output_3 = text_encoder_3(text_input_ids_3)
                uncond_output_3 = text_encoder_3(uncond_input_ids_3)
                
                # T5 ì¶œë ¥ ì²˜ë¦¬
                if hasattr(encoder_output_3, 'last_hidden_state'):
                    encoder_output_3 = encoder_output_3.last_hidden_state
                    uncond_output_3 = uncond_output_3.last_hidden_state
                elif isinstance(encoder_output_3, tuple) and len(encoder_output_3) > 0:
                    encoder_output_3 = encoder_output_3[0]
                    uncond_output_3 = uncond_output_3[0]
                
                # ì‹œí€€ìŠ¤ ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
                if isinstance(prompt_embeds, torch.Tensor) and isinstance(encoder_output_3, torch.Tensor):
                    if encoder_output_3.shape[1] == prompt_embeds.shape[1]:
                        # ì„ë² ë”© ì°¨ì› ê²°í•©
                        prompt_embeds = torch.cat([prompt_embeds, encoder_output_3], dim=-1)
                        negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_output_3], dim=-1)
                    else:
                        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°
                        min_seq_len = min(prompt_embeds.shape[1], encoder_output_3.shape[1])
                        prompt_embeds = prompt_embeds[:, :min_seq_len, :]
                        negative_prompt_embeds = negative_prompt_embeds[:, :min_seq_len, :]
                        encoder_output_3 = encoder_output_3[:, :min_seq_len, :]
                        uncond_output_3 = uncond_output_3[:, :min_seq_len, :]
                        
                        # ì„ë² ë”© ì°¨ì› ê²°í•©
                        prompt_embeds = torch.cat([prompt_embeds, encoder_output_3], dim=-1)
                        negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_output_3], dim=-1)
            except Exception as e:
                print(f"í…ìŠ¤íŠ¸ ì¸ì½”ë” 3 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # pooled embeddings ì²˜ë¦¬ (ì¤‘ìš”: ì´ ë¶€ë¶„ì´ timesteps_embì™€ ë”í•´ì§€ëŠ” ë¶€ë¶„)
        if pooled_prompt_embeds is None and isinstance(prompt_embeds, torch.Tensor):
            # pooled_embedsê°€ Noneì¸ ê²½ìš° ì²« ë²ˆì§¸ í† í° ì‚¬ìš©
            pooled_prompt_embeds = prompt_embeds[:, 0]  # (B, D)
            negative_pooled_prompt_embeds = negative_prompt_embeds[:, 0]  # (B, D)
        
        # pooled_embeds ì°¨ì› í™•ì¸ - ë°˜ë“œì‹œ (B, D) í˜•íƒœì—¬ì•¼ í•¨
        if isinstance(pooled_prompt_embeds, torch.Tensor):
            if len(pooled_prompt_embeds.shape) != 2:
                if len(pooled_prompt_embeds.shape) == 3:  # (B, S, D) í˜•íƒœì¸ ê²½ìš°
                    # ì²« ë²ˆì§¸ í† í°ë§Œ ì‚¬ìš© (B, S, D) -> (B, D)
                    pooled_prompt_embeds = pooled_prompt_embeds[:, 0]
                    negative_pooled_prompt_embeds = negative_pooled_prompt_embeds[:, 0]
                elif len(pooled_prompt_embeds.shape) == 1:  # (D) í˜•íƒœì¸ ê²½ìš°
                    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (D) -> (B, D)
                    pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0).repeat(batch_size, 1)
                    negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.unsqueeze(0).repeat(batch_size, 1)
        
            # pooled_embeds ì°¨ì› ê²€ì¦ ë° ìˆ˜ì • (SD3 í‘œì¤€: 2048)
            expected_pooled_dim = 2048
            if pooled_prompt_embeds.shape[-1] != expected_pooled_dim:
                if pooled_prompt_embeds.shape[-1] > expected_pooled_dim:
                    # ì°¨ì›ì´ í° ê²½ìš° ìë¥´ê¸°
                    pooled_prompt_embeds = pooled_prompt_embeds[:, :expected_pooled_dim]
                    negative_pooled_prompt_embeds = negative_pooled_prompt_embeds[:, :expected_pooled_dim]
                else:
                    # ì°¨ì›ì´ ì‘ì€ ê²½ìš° íŒ¨ë”©
                    current_dim = pooled_prompt_embeds.shape[-1]
                    padding_size = expected_pooled_dim - current_dim
                    pooled_prompt_embeds = F.pad(pooled_prompt_embeds, (0, padding_size))
                    negative_pooled_prompt_embeds = F.pad(negative_pooled_prompt_embeds, (0, padding_size))
        
        # encoder_hidden_states ì°¨ì› ê²€ì¦ ë° ìˆ˜ì • (SD3 í‘œì¤€: 4096)
        if isinstance(prompt_embeds, torch.Tensor):
            expected_encoder_dim = 4096
            if prompt_embeds.shape[-1] != expected_encoder_dim:
                if prompt_embeds.shape[-1] > expected_encoder_dim:
                    # ì°¨ì›ì´ í° ê²½ìš° ìë¥´ê¸°
                    prompt_embeds = prompt_embeds[:, :, :expected_encoder_dim]
                    negative_prompt_embeds = negative_prompt_embeds[:, :, :expected_encoder_dim]
                else:
                    # ì°¨ì›ì´ ì‘ì€ ê²½ìš° íŒ¨ë”©
                    current_dim = prompt_embeds.shape[-1]
                    padding_size = expected_encoder_dim - current_dim
                    prompt_embeds = F.pad(prompt_embeds, (0, padding_size))
                    negative_prompt_embeds = F.pad(negative_prompt_embeds, (0, padding_size))
        
        return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds

    def inference(self, texted_images_to_inpaint: list[TextedImage]) -> list[TextedImage]:
        """
        SD3 LoRA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° TextedImageì˜ ë§ˆìŠ¤í¬ëœ ì˜ì—­ì„ ì¸í˜ì¸íŒ…

        Args:
            texted_images_to_inpaint: ì¸í˜ì¸íŒ…í•  TextedImage ê°ì²´ ë¦¬ìŠ¤íŠ¸
                                     ê° ê°ì²´ëŠ” ì›ë³¸ ì´ë¯¸ì§€ì˜ íŠ¹ì • bbox ì£¼ë³€ì„ center-croppedí•œ íŒ¨ì¹˜

        Returns:
            list[TextedImage]: ì¸í˜ì¸íŒ…ëœ ê²°ê³¼ê°€ origì— ì €ì¥ëœ TextedImage ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        if len(texted_images_to_inpaint) == 0:
            print("No images to process for Model3 inference.")
            return []

        print("Loading SD3 pipeline for inference...")

        # 1. íŒŒì´í”„ë¼ì¸ ë° ëª¨ë¸ ë¡œë”©
        model_id = self.model_config["model_id"]
        lora_weights_path = self.model_config["lora_path"]

        try:
            # SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ê¸°ë³¸ fp16)
            pipe = StableDiffusion3Pipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16,  # Transformer, Text EncodersëŠ” fp16
            )

            # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ì ìš©
            if os.path.exists(lora_weights_path):
                transformer = pipe.transformer
                transformer = PeftModel.from_pretrained(transformer, lora_weights_path)
                pipe.transformer = transformer
                print(f"LoRA weights loaded from {lora_weights_path}")
            else:
                print(f"Warning: LoRA weights not found at {lora_weights_path}, using base model")

            # íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™
            pipe.to(self.device)

            # VAEë§Œ fp32ë¡œ ë³€ê²½ (ì¬êµ¬ì„± í’ˆì§ˆ í–¥ìƒ)
            pipe.vae = pipe.vae.to(dtype=torch.float32)

            # ì¶”ë¡  ì„¤ì •
            prompt = self.model_config.get("prompts", "pure black and white manga style image with no color tint, absolute grayscale, contextual manga style")
            negative_prompt = self.model_config.get("negative_prompt", "photo, realistic, color, colorful, purple, violet, sepia, any color tint, blurry")
            guidance_scale = self.model_config.get("guidance_scale", 7.5)
            num_inference_steps = self.model_config.get("inference_steps", 5)

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            output_dir = self.model_config.get("output_dir", "trit/datas/images/output")
            os.makedirs(output_dir, exist_ok=True)

            # 2. ê° íŒ¨ì¹˜ TextedImageì— ëŒ€í•œ ë°˜ë³µ ì²˜ë¦¬
            for i, current_patch_texted_image in enumerate(tqdm(texted_images_to_inpaint, desc="Inpainting patches")):
                try:
                    # VRAM ê´€ë¦¬ë¥¼ ìœ„í•´ ê° íŒ¨ì¹˜ ì²˜ë¦¬ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()

                    # A. ì…ë ¥ ì¤€ë¹„ (íŒ¨ì¹˜ ë‹¨ìœ„)
                    # íŒ¨ì¹˜ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ PILë¡œ ë³€í™˜
                    patch_orig_pil = transforms.ToPILImage()(current_patch_texted_image.orig.cpu())
                    patch_mask_pil = transforms.ToPILImage()(current_patch_texted_image.mask.cpu().squeeze(0))  # (1,H,W) -> (H,W)

                    # ë§ˆìŠ¤í¬ ì´ì§„í™” (0 ë˜ëŠ” 255)
                    mask_np = np.array(patch_mask_pil)
                    mask_binary = (mask_np > 127).astype(np.uint8) * 255
                    mask_binary_pil = Image.fromarray(mask_binary, "L")

                    # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ [-1,1] ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ê³  fp32ë¡œ ë³€í™˜ (VAEìš©)
                    init_image_for_vae = transforms.ToTensor()(patch_orig_pil).unsqueeze(0)  # [1,C,H,W]
                    init_image_for_vae = (init_image_for_vae * 2.0 - 1.0).to(self.device, dtype=torch.float32)

                    # ë§ˆìŠ¤í¬ë¥¼ fp16ìœ¼ë¡œ ë³€í™˜ (Transformerìš©)
                    mask_tensor_for_transformer = transforms.ToTensor()(mask_binary_pil).unsqueeze(0)  # [1,1,H,W]
                    mask_tensor_for_transformer = mask_tensor_for_transformer.to(self.device, dtype=torch.float16)

                    with torch.no_grad():
                        # B. VAE ì¸ì½”ë”© (fp32 ì—°ì‚°)
                        latent_image = pipe.vae.encode(init_image_for_vae).latent_dist.sample()
                        latent_image = latent_image * pipe.vae.config.scaling_factor  # fp32

                        # C. Latent ë§ˆìŠ¤í¬ ì¤€ë¹„
                        latent_mask_for_transformer = F.interpolate(
                            mask_tensor_for_transformer,
                            size=latent_image.shape[-2:],  # latent í•´ìƒë„ì— ë§ì¶¤
                            mode="nearest"
                        )  # fp16

                        # D. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (fp16 ì—°ì‚°)
                        try:
                            # SD3 íŒŒì´í”„ë¼ì¸ì˜ ë‚´ì¥ ì¸ì½”ë”© ë©”ì„œë“œ ì‚¬ìš©
                            (
                                prompt_embeds,
                                negative_prompt_embeds,
                                pooled_prompt_embeds,
                                negative_pooled_prompt_embeds,
                            ) = pipe.encode_prompt(
                                prompt=prompt,
                                prompt_2=prompt,  # SD3ëŠ” ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ë°›ìŒ
                                prompt_3=prompt,
                                device=self.device,
                                num_images_per_prompt=1,
                                do_classifier_free_guidance=True,
                                negative_prompt=negative_prompt,
                                negative_prompt_2=negative_prompt,
                                negative_prompt_3=negative_prompt,
                            )

                            # CFGë¥¼ ìœ„í•´ ì„ë² ë”© ê²°í•© (None ì²´í¬ ì¶”ê°€)
                            if negative_prompt_embeds is not None and prompt_embeds is not None:
                                prompt_embeds_cfg = torch.cat([negative_prompt_embeds, prompt_embeds])  # fp16
                            else:
                                raise ValueError("prompt_embeds ë˜ëŠ” negative_prompt_embedsê°€ Noneì…ë‹ˆë‹¤.")

                            if negative_pooled_prompt_embeds is not None and pooled_prompt_embeds is not None:
                                pooled_embeds_cfg = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds])  # fp16
                            else:
                                # pooled embedsê°€ Noneì¸ ê²½ìš° ì²« ë²ˆì§¸ í† í° ì‚¬ìš©
                                pooled_embeds_cfg = prompt_embeds_cfg[:, 0]

                        except Exception as e:
                            print(f"Text encoding error for patch {i+1}: {e}")
                            # ëŒ€ì²´ ë°©ë²•: ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§•
                            text_inputs = pipe.tokenizer(
                                [negative_prompt, prompt],
                                padding="max_length",
                                max_length=77,
                                truncation=True,
                                return_tensors="pt",
                            ).to(self.device)

                            prompt_embeds_cfg = pipe.text_encoder(text_inputs.input_ids)[0]  # fp16
                            pooled_embeds_cfg = prompt_embeds_cfg[:, 0]  # ì²« ë²ˆì§¸ í† í°ì„ pooledë¡œ ì‚¬ìš©

                        # E. ì¸í˜ì¸íŒ…ì„ ìœ„í•œ ì´ˆê¸° Latent ì¤€ë¹„
                        noise = torch.randn_like(latent_image, dtype=latent_image.dtype)  # fp32
                        latent_mask_for_compositing = latent_mask_for_transformer.to(
                            dtype=latent_image.dtype, device=latent_image.device
                        )  # fp32ë¡œ ë³€í™˜
                        
                        

                        # ë§ˆìŠ¤í¬ëœ ì˜ì—­ì—ë§Œ ë…¸ì´ì¦ˆ ì ìš©
                        latents = latent_image * (1 - latent_mask_for_compositing) + noise * latent_mask_for_compositing # resizeing 

                        # F. ë””í“¨ì „ ë£¨í”„ (Denoising Loop)
                        pipe.scheduler.set_timesteps(num_inference_steps, device=self.device)
                        timesteps = pipe.scheduler.timesteps

                        for t in timesteps:
                            # CFGë¥¼ ìœ„í•´ latents ë³µì œí•˜ê³  Transformer ì…ë ¥ ì •ë°€ë„(fp16)ë¡œ ë³€í™˜
                            latent_model_input_cfg = torch.cat([latents] * 2).to(dtype=torch.float16)

                            # íƒ€ì„ìŠ¤í… ì¤€ë¹„
                            t_input = t.expand(latents.shape[0])

                            # Transformerë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ (fp16 ì—°ì‚°)
                            noise_pred_transformer = pipe.transformer(
                                hidden_states=latent_model_input_cfg,
                                timestep=t_input,
                                encoder_hidden_states=prompt_embeds_cfg,
                                pooled_projections=pooled_embeds_cfg,
                                return_dict=False
                            )[0]  # fp16

                            # CFG ì ìš©
                            noise_pred_uncond, noise_pred_cond = noise_pred_transformer.chunk(2)
                            noise_pred_cfg = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)

                            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (fp32ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬)
                            latents = pipe.scheduler.step(
                                noise_pred_cfg.to(dtype=latents.dtype), t, latents, return_dict=False
                            )[0]  # fp32 ìœ ì§€

                            # ë§¥ë½ ì¬ì£¼ì… (ì¤‘ìš”: ë§ˆìŠ¤í¬ë˜ì§€ ì•Šì€ ì˜ì—­ì€ ì›ë³¸ ìœ ì§€)
                            latents = latent_image * (1 - latent_mask_for_compositing) + latents * latent_mask_for_compositing

                        # G. VAE ë””ì½”ë”© (fp32 ì—°ì‚°)
                        image_tensor_pred = pipe.vae.decode(
                            latents / pipe.vae.config.scaling_factor, return_dict=False
                        )[0]  # fp32, [-1,1] ë²”ìœ„
                        
                        print(image_tensor_pred.shape)

                        # H. í›„ì²˜ë¦¬ ë° TextedImage ê°ì²´ ì—…ë°ì´íŠ¸
                        # [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”
                        image_tensor_pred = (image_tensor_pred / 2 + 0.5).clamp(0, 1)

                        # [1,C,H,W] -> [C,H,W]ë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ dtype/deviceë¡œ ë§ì¶¤
                        final_tensor = image_tensor_pred.squeeze(0).to(
                            dtype=current_patch_texted_image.orig.dtype,
                            device=current_patch_texted_image.orig.device
                        )

                        # ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ ì ìš©í•˜ê¸° : ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬ í›„ ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ ì¸í˜ì¸íŒ… ê²°ê³¼ë¡œ ëŒ€ì²´
                        mask_for_compositing = current_patch_texted_image.mask.to(
                            dtype=final_tensor.dtype,
                            device=final_tensor.device
                        ) 
                        original_tensor = current_patch_texted_image.orig.clone()
                        composited_result = original_tensor * (1 - mask_for_compositing) + final_tensor * mask_for_compositing

                        # I. íŒ¨ì¹˜ í¬ê¸° ì¡°ì • (ì¤‘ìš”: ì›ë³¸ bbox í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ) --ì¬ê²€í† í† 
                        # í˜„ì¬ íŒ¨ì¹˜ì˜ _bbox (center cropëœ ì¢Œí‘œ)
                        # patch_bbox = current_patch_texted_image.bboxes[0]

                        # íŒ¨ì¹˜ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì˜ì—­ë§Œ ì¶”ì¶œ (center cropì—ì„œ ì˜ë¦° ë¶€ë¶„ ì œê±°)
                        # cropped_final_tensor = final_tensor[patch_bbox.slice]

                        # TextedImage ê°ì²´ì˜ orig ì†ì„± ì—…ë°ì´íŠ¸ (croppedëœ ë²„ì „ìœ¼ë¡œ)
                        current_patch_texted_image.orig = composited_result
                        
                        # orig = final_tensor
                        
                        # a = TextedImage(orig, orig, torch.zeros(1, orig.shape[1], orig.shape[2],), current_patch_texted_image.bboxes)
                        # a.visualize(dir="trit/datas/images/output", filename=f"inpainting.png")

                        print(f"Successfully inpainted patch {i+1}/{len(texted_images_to_inpaint)}")

                except Exception as e:
                    print(f"Error during inference for patch {i+1}: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ íŒ¨ì¹˜ ìœ ì§€
                    continue

            # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬
            del pipe
            gc.collect()
            torch.cuda.empty_cache()

            print(f"Inference completed. Results saved to {output_dir}")
            return texted_images_to_inpaint

        except Exception as e:
            print(f"Error initializing SD3 pipeline: {e}")
            return texted_images_to_inpaint

















